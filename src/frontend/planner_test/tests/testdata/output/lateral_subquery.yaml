# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- name: lateral join 1
  sql: |
    create table all_sales (salesperson_id int, customer_name varchar, amount int );
    create table salesperson (id int, name varchar );
    SELECT
    salesperson.name,
    max_sale.amount,
    max_sale_customer.customer_name
    FROM
    salesperson,
    -- calculate maximum size, cache it in transient derived table max_sale
    LATERAL
    (SELECT MAX(amount) AS amount
    FROM all_sales
    WHERE all_sales.salesperson_id = salesperson.id)
    AS max_sale,
    -- find customer, reusing cached maximum size
    LATERAL
    (SELECT customer_name
    FROM all_sales
    WHERE all_sales.salesperson_id = salesperson.id
    AND all_sales.amount =
    -- the cached maximum size
    max_sale.amount)
    AS max_sale_customer;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashJoin { type: Inner, predicate: salesperson.id = all_sales.salesperson_id AND max(all_sales.amount) = all_sales.amount, output: [salesperson.name, max(all_sales.amount), all_sales.customer_name] }
      ├─BatchExchange { order: [], dist: HashShard(salesperson.id, max(all_sales.amount)) }
      │ └─BatchHashJoin { type: Inner, predicate: salesperson.id = all_sales.salesperson_id, output: [salesperson.id, salesperson.name, max(all_sales.amount)] }
      │   ├─BatchExchange { order: [], dist: HashShard(salesperson.id) }
      │   │ └─BatchScan { table: salesperson, columns: [salesperson.id, salesperson.name], distribution: SomeShard }
      │   └─BatchHashAgg { group_key: [all_sales.salesperson_id], aggs: [max(all_sales.amount)] }
      │     └─BatchExchange { order: [], dist: HashShard(all_sales.salesperson_id) }
      │       └─BatchScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.amount], distribution: SomeShard }
      └─BatchExchange { order: [], dist: HashShard(all_sales.salesperson_id, all_sales.amount) }
        └─BatchScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.customer_name, all_sales.amount], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [name, amount, customer_name, salesperson._row_id(hidden), salesperson.id(hidden), all_sales._row_id(hidden)], stream_key: [salesperson._row_id, salesperson.id, all_sales._row_id, amount], pk_columns: [salesperson._row_id, salesperson.id, all_sales._row_id, amount], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(max(all_sales.amount), salesperson._row_id, salesperson.id, all_sales._row_id) }
      └─StreamHashJoin { type: Inner, predicate: salesperson.id = all_sales.salesperson_id AND max(all_sales.amount) = all_sales.amount, output: [salesperson.name, max(all_sales.amount), all_sales.customer_name, salesperson._row_id, salesperson.id, all_sales._row_id] }
        ├─StreamExchange { dist: HashShard(salesperson.id, max(all_sales.amount)) }
        │ └─StreamHashJoin { type: Inner, predicate: salesperson.id = all_sales.salesperson_id, output: [salesperson.id, salesperson.name, max(all_sales.amount), salesperson._row_id, all_sales.salesperson_id] }
        │   ├─StreamExchange { dist: HashShard(salesperson.id) }
        │   │ └─StreamTableScan { table: salesperson, columns: [salesperson.id, salesperson.name, salesperson._row_id], stream_scan_type: ArrangementBackfill, pk: [salesperson._row_id], dist: UpstreamHashShard(salesperson._row_id) }
        │   └─StreamProject { exprs: [all_sales.salesperson_id, max(all_sales.amount)] }
        │     └─StreamHashAgg { group_key: [all_sales.salesperson_id], aggs: [max(all_sales.amount), count] }
        │       └─StreamExchange { dist: HashShard(all_sales.salesperson_id) }
        │         └─StreamTableScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.amount, all_sales._row_id], stream_scan_type: ArrangementBackfill, pk: [all_sales._row_id], dist: UpstreamHashShard(all_sales._row_id) }
        └─StreamExchange { dist: HashShard(all_sales.salesperson_id, all_sales.amount) }
          └─StreamTableScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.customer_name, all_sales.amount, all_sales._row_id], stream_scan_type: ArrangementBackfill, pk: [all_sales._row_id], dist: UpstreamHashShard(all_sales._row_id) }
- name: lateral join 2
  sql: |
    create table all_sales (salesperson_id int, customer_name varchar, amount int );
    create table salesperson (id int, name varchar );
    SELECT
    salesperson.name,
    max_sale.amount,
    max_sale.customer_name
    FROM
    salesperson,
    -- find maximum size and customer at same time
    LATERAL
    (SELECT amount, customer_name
    FROM all_sales
    WHERE all_sales.salesperson_id = salesperson.id
    ORDER BY amount DESC LIMIT 1)
    AS max_sale;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashJoin { type: Inner, predicate: salesperson.id IS NOT DISTINCT FROM all_sales.salesperson_id, output: [salesperson.name, all_sales.amount, all_sales.customer_name] }
      ├─BatchExchange { order: [], dist: HashShard(salesperson.id) }
      │ └─BatchScan { table: salesperson, columns: [salesperson.id, salesperson.name], distribution: SomeShard }
      └─BatchGroupTopN { order: [all_sales.amount DESC], limit: 1, offset: 0, group_key: [all_sales.salesperson_id] }
        └─BatchExchange { order: [], dist: HashShard(all_sales.salesperson_id) }
          └─BatchProject { exprs: [all_sales.salesperson_id, all_sales.amount, all_sales.customer_name] }
            └─BatchFilter { predicate: IsNotNull(all_sales.salesperson_id) }
              └─BatchScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.customer_name, all_sales.amount], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [name, amount, customer_name, salesperson._row_id(hidden), salesperson.id(hidden), all_sales.salesperson_id(hidden)], stream_key: [salesperson._row_id, salesperson.id], pk_columns: [salesperson._row_id, salesperson.id], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(salesperson._row_id, salesperson.id) }
      └─StreamHashJoin { type: Inner, predicate: salesperson.id IS NOT DISTINCT FROM all_sales.salesperson_id, output: [salesperson.name, all_sales.amount, all_sales.customer_name, salesperson._row_id, salesperson.id, all_sales.salesperson_id] }
        ├─StreamExchange { dist: HashShard(salesperson.id) }
        │ └─StreamTableScan { table: salesperson, columns: [salesperson.id, salesperson.name, salesperson._row_id], stream_scan_type: ArrangementBackfill, pk: [salesperson._row_id], dist: UpstreamHashShard(salesperson._row_id) }
        └─StreamGroupTopN { order: [all_sales.amount DESC], limit: 1, offset: 0, group_key: [all_sales.salesperson_id] }
          └─StreamExchange { dist: HashShard(all_sales.salesperson_id) }
            └─StreamProject { exprs: [all_sales.salesperson_id, all_sales.amount, all_sales.customer_name, all_sales._row_id] }
              └─StreamFilter { predicate: IsNotNull(all_sales.salesperson_id) }
                └─StreamTableScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.customer_name, all_sales.amount, all_sales._row_id], stream_scan_type: ArrangementBackfill, pk: [all_sales._row_id], dist: UpstreamHashShard(all_sales._row_id) }
- name: lateral join 2 (left join)
  sql: |
    create table all_sales (salesperson_id int, customer_name varchar, amount int );
    create table salesperson (id int, name varchar );
    SELECT
    salesperson.name,
    max_sale.amount,
    max_sale.customer_name
    FROM
    salesperson LEFT JOIN
    -- find maximum size and customer at same time
    LATERAL
    (SELECT amount, customer_name
    FROM all_sales
    WHERE all_sales.salesperson_id = salesperson.id
    ORDER BY amount DESC LIMIT 1)
    AS max_sale on true;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashJoin { type: LeftOuter, predicate: salesperson.id IS NOT DISTINCT FROM all_sales.salesperson_id, output: [salesperson.name, all_sales.amount, all_sales.customer_name] }
      ├─BatchExchange { order: [], dist: HashShard(salesperson.id) }
      │ └─BatchScan { table: salesperson, columns: [salesperson.id, salesperson.name], distribution: SomeShard }
      └─BatchGroupTopN { order: [all_sales.amount DESC], limit: 1, offset: 0, group_key: [all_sales.salesperson_id] }
        └─BatchExchange { order: [], dist: HashShard(all_sales.salesperson_id) }
          └─BatchProject { exprs: [all_sales.salesperson_id, all_sales.amount, all_sales.customer_name] }
            └─BatchFilter { predicate: IsNotNull(all_sales.salesperson_id) }
              └─BatchScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.customer_name, all_sales.amount], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [name, amount, customer_name, salesperson._row_id(hidden), salesperson.id(hidden), all_sales.salesperson_id(hidden)], stream_key: [salesperson._row_id, salesperson.id], pk_columns: [salesperson._row_id, salesperson.id], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(salesperson._row_id, salesperson.id) }
      └─StreamHashJoin { type: LeftOuter, predicate: salesperson.id IS NOT DISTINCT FROM all_sales.salesperson_id, output: [salesperson.name, all_sales.amount, all_sales.customer_name, salesperson._row_id, salesperson.id, all_sales.salesperson_id] }
        ├─StreamExchange { dist: HashShard(salesperson.id) }
        │ └─StreamTableScan { table: salesperson, columns: [salesperson.id, salesperson.name, salesperson._row_id], stream_scan_type: ArrangementBackfill, pk: [salesperson._row_id], dist: UpstreamHashShard(salesperson._row_id) }
        └─StreamGroupTopN { order: [all_sales.amount DESC], limit: 1, offset: 0, group_key: [all_sales.salesperson_id] }
          └─StreamExchange { dist: HashShard(all_sales.salesperson_id) }
            └─StreamProject { exprs: [all_sales.salesperson_id, all_sales.amount, all_sales.customer_name, all_sales._row_id] }
              └─StreamFilter { predicate: IsNotNull(all_sales.salesperson_id) }
                └─StreamTableScan { table: all_sales, columns: [all_sales.salesperson_id, all_sales.customer_name, all_sales.amount, all_sales._row_id], stream_scan_type: ArrangementBackfill, pk: [all_sales._row_id], dist: UpstreamHashShard(all_sales._row_id) }
- name: lateral join 2 (right join) should throw an error
  sql: |
    create table all_sales (salesperson_id int, customer_name varchar, amount int );
    create table salesperson (id int, name varchar );
    SELECT
    salesperson.name,
    max_sale.amount,
    max_sale.customer_name
    FROM
    salesperson RIGHT JOIN
    -- find maximum size and customer at same time
    LATERAL
    (SELECT amount, customer_name
    FROM all_sales
    WHERE all_sales.salesperson_id = salesperson.id
    ORDER BY amount DESC LIMIT 1)
    AS max_sale on true;
  binder_error: 'Invalid input syntax: The combining JOIN type must be INNER or LEFT for a LATERAL reference.'
- name: implicit lateral subquery of correlated table function
  sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, Unnest($0)] }
      ├─BatchExchange { order: [], dist: HashShard(t.arr) }
      │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
      └─BatchProjectSet { select_list: [$0, Unnest($0)] }
        └─BatchHashAgg { group_key: [t.arr], aggs: [] }
          └─BatchExchange { order: [], dist: HashShard(t.arr) }
            └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, unnest, t._row_id(hidden), t.arr(hidden), projected_row_id(hidden)], stream_key: [t._row_id, projected_row_id, arr], pk_columns: [t._row_id, projected_row_id, arr], pk_conflict: NoCheck }
    └─StreamExchange { dist: HashShard(t.arr, t._row_id, projected_row_id) }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, Unnest($0), t._row_id, t.arr, projected_row_id] }
        ├─StreamExchange { dist: HashShard(t.arr) }
        │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, pk: [t._row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamExchange { dist: HashShard(t.arr) }
                └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, pk: [t._row_id], dist: UpstreamHashShard(t._row_id) }
- name: https://github.com/risingwavelabs/risingwave/issues/12298
  sql: |
    create table t1(c varchar, n varchar, id varchar, d varchar);
    create table t2(c varchar, p varchar, id varchar, d varchar);
    select array_agg(t1.n order by path_idx) from t1
    join t2
    on t1.c = 'abc'
    and t2.c = 'abc'
    cross join unnest((case when t2.p <> '' then (string_to_array(trim(t2.p, ','), ',') || t2.d) else ARRAY[t2.d] end)) WITH ORDINALITY AS path_cols(path_val, path_idx)
    where path_val = t1.id;
  stream_plan: |-
    StreamMaterialize { columns: [array_agg], stream_key: [], pk_columns: [], pk_conflict: NoCheck }
    └─StreamProject { exprs: [array_agg(t1.n order_by($expr1 ASC))] }
      └─StreamSimpleAgg { aggs: [array_agg(t1.n order_by($expr1 ASC)), count] }
        └─StreamExchange { dist: Single }
          └─StreamProject { exprs: [t1.n, (projected_row_id + 1:Int64) as $expr1, t1._row_id, t2.p, t2.p, t2.d, t2.d, projected_row_id, t1.id, t2._row_id] }
            └─StreamHashJoin { type: Inner, predicate: t2.p IS NOT DISTINCT FROM t2.p AND t2.p IS NOT DISTINCT FROM t2.p AND t2.d IS NOT DISTINCT FROM t2.d AND t2.d IS NOT DISTINCT FROM t2.d, output: [t1.n, t1.id, projected_row_id, t2.p, t2.p, t2.d, t2.d, Unnest(Case(($1 <> '':Varchar), ArrayAppend(StringToArray(Trim($1, ',':Varchar), ',':Varchar), $3), Array($3))), t2.p, t2.d, t1._row_id, t2._row_id] }
              ├─StreamExchange { dist: HashShard(t2.p, t2.d) }
              │ └─StreamHashJoin { type: Inner, predicate: t1.id = Unnest(Case(($1 <> '':Varchar), ArrayAppend(StringToArray(Trim($1, ',':Varchar), ',':Varchar), $3), Array($3))), output: [t1.n, t1.id, projected_row_id, t2.p, t2.p, t2.d, t2.d, Unnest(Case(($1 <> '':Varchar), ArrayAppend(StringToArray(Trim($1, ',':Varchar), ',':Varchar), $3), Array($3))), t1._row_id] }
              │   ├─StreamExchange { dist: HashShard(t1.id) }
              │   │ └─StreamProject { exprs: [t1.n, t1.id, t1._row_id] }
              │   │   └─StreamFilter { predicate: (t1.c = 'abc':Varchar) }
              │   │     └─StreamTableScan { table: t1, columns: [t1.n, t1.id, t1._row_id, t1.c], stream_scan_type: ArrangementBackfill, pk: [t1._row_id], dist: UpstreamHashShard(t1._row_id) }
              │   └─StreamExchange { dist: HashShard(Unnest(Case(($1 <> '':Varchar), ArrayAppend(StringToArray(Trim($1, ',':Varchar), ',':Varchar), $3), Array($3)))) }
              │     └─StreamProjectSet { select_list: [$0, $1, $2, $3, Unnest(Case(($1 <> '':Varchar), ArrayAppend(StringToArray(Trim($1, ',':Varchar), ',':Varchar), $3), Array($3)))] }
              │       └─StreamProject { exprs: [t2.p, t2.p, t2.d, t2.d] }
              │         └─StreamHashAgg { group_key: [t2.p, t2.p, t2.d, t2.d], aggs: [count] }
              │           └─StreamExchange { dist: HashShard(t2.p, t2.p, t2.d, t2.d) }
              │             └─StreamProject { exprs: [t2.p, t2.p, t2.d, t2.d, t2._row_id] }
              │               └─StreamFilter { predicate: (t2.c = 'abc':Varchar) }
              │                 └─StreamTableScan { table: t2, columns: [t2.p, t2.p, t2.d, t2.d, t2._row_id, t2.c], stream_scan_type: ArrangementBackfill, pk: [t2._row_id], dist: UpstreamHashShard(t2._row_id) }
              └─StreamExchange { dist: HashShard(t2.p, t2.d) }
                └─StreamProject { exprs: [t2.p, t2.d, t2._row_id] }
                  └─StreamFilter { predicate: (t2.c = 'abc':Varchar) }
                    └─StreamTableScan { table: t2, columns: [t2.p, t2.d, t2._row_id, t2.c], stream_scan_type: ArrangementBackfill, pk: [t2._row_id], dist: UpstreamHashShard(t2._row_id) }
